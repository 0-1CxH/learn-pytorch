{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa4c420f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df2ef924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__', '_image_backend', 'datasets', 'get_image_backend', 'models', 'set_image_backend', 'transforms', 'utils']\n"
     ]
    }
   ],
   "source": [
    "print(dir(torchvision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52b73c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CIFAR10', 'CIFAR100', 'Cityscapes', 'CocoCaptions', 'CocoDetection', 'DatasetFolder', 'EMNIST', 'FakeData', 'FashionMNIST', 'Flickr30k', 'Flickr8k', 'ImageFolder', 'KMNIST', 'LSUN', 'LSUNClass', 'MNIST', 'Omniglot', 'PhotoTour', 'SBU', 'SEMEION', 'STL10', 'SVHN', 'VOCDetection', 'VOCSegmentation', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'cifar', 'cityscapes', 'coco', 'fakedata', 'flickr', 'folder', 'lsun', 'mnist', 'omniglot', 'phototour', 'sbu', 'semeion', 'stl10', 'svhn', 'utils', 'voc']\n"
     ]
    }
   ],
   "source": [
    "print(dir(torchvision.datasets)) # torchvision.datasets includes famous CV datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d8426da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AlexNet', 'DenseNet', 'Inception3', 'ResNet', 'SqueezeNet', 'VGG', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'alexnet', 'densenet', 'densenet121', 'densenet161', 'densenet169', 'densenet201', 'inception', 'inception_v3', 'resnet', 'resnet101', 'resnet152', 'resnet18', 'resnet34', 'resnet50', 'squeezenet', 'squeezenet1_0', 'squeezenet1_1', 'vgg', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19', 'vgg19_bn']\n"
     ]
    }
   ],
   "source": [
    "print(dir(torchvision.models))  # torchvision.models includes famous CV models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0ebd35b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CenterCrop', 'ColorJitter', 'Compose', 'FiveCrop', 'Grayscale', 'Lambda', 'LinearTransformation', 'Normalize', 'Pad', 'RandomAffine', 'RandomApply', 'RandomChoice', 'RandomCrop', 'RandomGrayscale', 'RandomHorizontalFlip', 'RandomOrder', 'RandomResizedCrop', 'RandomRotation', 'RandomSizedCrop', 'RandomVerticalFlip', 'Resize', 'Scale', 'TenCrop', 'ToPILImage', 'ToTensor', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'functional', 'transforms']\n"
     ]
    }
   ],
   "source": [
    "print(dir(torchvision.transforms)) # torchvision.transforms includes common transformations on images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11b1da69",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_transformation = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.5,), (0.5,))  # normalize: each pixel=(pixel-0.5)/0.5\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c45d4f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Split: train\n",
      "    Root Location: ./torchvision_data\n",
      "    Transforms (if any): Compose(\n",
      "                             ToTensor()\n",
      "                             Normalize(mean=(0.5,), std=(0.5,))\n",
      "                         )\n",
      "    Target Transforms (if any): None\n",
      "Dataset MNIST\n",
      "    Number of datapoints: 10000\n",
      "    Split: test\n",
      "    Root Location: ./torchvision_data\n",
      "    Transforms (if any): Compose(\n",
      "                             ToTensor()\n",
      "                             Normalize(mean=(0.5,), std=(0.5,))\n",
      "                         )\n",
      "    Target Transforms (if any): None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_dataset = torchvision.datasets.mnist\n",
    "training_set = mnist_dataset.MNIST('./torchvision_data', train=True, transform=mnist_transformation)  # if not downloaded, set download=True\n",
    "test_set = mnist_dataset.MNIST('./torchvision_data', train=False, transform=mnist_transformation) # apply transform to each img in the dataset\n",
    "print(training_set), print(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "907d5ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareMNISTData:\n",
    "    batch = 32\n",
    "    training_data = DataLoader(\n",
    "        dataset=training_set,\n",
    "        batch_size=PrepareMNISTData.batch,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    test_data = DataLoader(\n",
    "        dataset=test_set,\n",
    "        batch_size=PrepareMNISTData.batch,\n",
    "        shuffle=True\n",
    "    )\n",
    "    @staticmethod\n",
    "    def get_training_set():\n",
    "        return iter(PrepareMNISTData.training_data)\n",
    "    @staticmethod\n",
    "    def get_test_set():\n",
    "        return iter(PrepareMNISTData.test_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c71d1cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = next(PrepareMNISTData.get_training_set())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "32dfec46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 28, 28]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "print(a[0].shape, a[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "8fda9c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FourLayerLinearNeuralNetwork:\n",
    "    def __init__(self, lr=5e-2, epoch=20):\n",
    "        \n",
    "        self.network = torch.nn.Sequential(\n",
    "            torch.nn.Linear(784, 400),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(400, 200),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(200, 100),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(100, 10),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        for layer in self.network:\n",
    "            if isinstance(layer, torch.nn.Linear):\n",
    "                torch.nn.init.xavier_uniform_(layer.weight)\n",
    "        \n",
    "        self.epoch = epoch\n",
    "        \n",
    "        self.optimizer = torch.optim.SGD(self.network.parameters(), lr = lr)\n",
    "        self.loss_func = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "    def train(self, get_training_data_iter_function):\n",
    "        self.network.train()\n",
    "        for e in range(self.epoch):\n",
    "            loss_in_epoch = 0\n",
    "            batch_count = 0\n",
    "            items_in_epoch = 0\n",
    "            correct_items_in_epoch = 0\n",
    "            \n",
    "            for data_batch in get_training_data_iter_function():\n",
    "                imgs, lbls = data_batch\n",
    "                imgs = imgs.reshape(imgs.shape[0], -1)\n",
    "                lbls = lbls.long() #torch.nn.functional.one_hot(lbls).float()\n",
    "                \n",
    "                out = self.network(Variable(imgs))\n",
    "                loss = self.loss_func(out, Variable(lbls))\n",
    "                \n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                _,pred = out.max(1)\n",
    "                correct_items_in_epoch+= (pred==lbls).sum().item()\n",
    "                items_in_epoch += imgs.shape[0]\n",
    "                \n",
    "                loss_in_epoch += loss.data.item()\n",
    "                batch_count += 1\n",
    "                \n",
    "            print(\"epoch{}/{}, training loss={}, accuracy={}\".format(e, self.epoch, \n",
    "                                                                     loss_in_epoch/batch_count, \n",
    "                                                                     correct_items_in_epoch/items_in_epoch))\n",
    "    \n",
    "    def evaluate(self, get_test_data_iter_function):\n",
    "        self.network.eval()\n",
    "        loss_in_epoch = 0\n",
    "        batch_count = 0\n",
    "        items_in_epoch = 0\n",
    "        correct_items_in_epoch = 0\n",
    "        \n",
    "        for data_batch in get_test_data_iter_function():\n",
    "            imgs, lbls = data_batch\n",
    "            imgs = imgs.reshape(imgs.shape[0], -1)\n",
    "            lbls = lbls.long()\n",
    "            \n",
    "            out = self.network(Variable(imgs))\n",
    "            loss = self.loss_func(out, Variable(lbls))\n",
    "            \n",
    "            _,pred = out.max(1)\n",
    "            correct_items_in_epoch+= (pred==lbls).sum().item()\n",
    "            items_in_epoch += imgs.shape[0]\n",
    "            \n",
    "            loss_in_epoch += loss.data.item()\n",
    "            batch_count += 1\n",
    "        \n",
    "        print(\"eval loss={}, accuracy={}\".format(loss_in_epoch/batch_count, correct_items_in_epoch/items_in_epoch))\n",
    "    \n",
    "    def predict(self, img_var):\n",
    "        return self.network(img_var).max(1)[1]\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "69e29ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "net =FourLayerLinearNeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "9bb368df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0/20, training loss=0.917797791258494, accuracy=0.6579166666666667\n",
      "epoch1/20, training loss=0.40618761148899796, accuracy=0.8505333333333334\n",
      "epoch2/20, training loss=0.3237276238349577, accuracy=0.8776333333333334\n",
      "epoch3/20, training loss=0.3036727732191483, accuracy=0.8824333333333333\n",
      "epoch4/20, training loss=0.2891933809529679, accuracy=0.88595\n",
      "epoch5/20, training loss=0.2785459199167322, accuracy=0.8885\n",
      "epoch6/20, training loss=0.2705649865209125, accuracy=0.8904833333333333\n",
      "epoch7/20, training loss=0.2643465009219789, accuracy=0.8920166666666667\n",
      "epoch8/20, training loss=0.25939889989804166, accuracy=0.8932833333333333\n",
      "epoch9/20, training loss=0.25308714145893074, accuracy=0.89505\n",
      "epoch10/20, training loss=0.25129529921258026, accuracy=0.8955333333333333\n",
      "epoch11/20, training loss=0.24846673242559772, accuracy=0.8961666666666667\n",
      "epoch12/20, training loss=0.24554003199531385, accuracy=0.8968333333333334\n",
      "epoch13/20, training loss=0.2419633799520069, accuracy=0.89775\n",
      "epoch14/20, training loss=0.2387473215508624, accuracy=0.8986166666666666\n",
      "epoch15/20, training loss=0.23902936480734208, accuracy=0.8983666666666666\n",
      "epoch16/20, training loss=0.23630238929068825, accuracy=0.89905\n",
      "epoch17/20, training loss=0.23737724090692647, accuracy=0.8988\n",
      "epoch18/20, training loss=0.2369323890719233, accuracy=0.8988333333333334\n",
      "epoch19/20, training loss=0.23653853395450278, accuracy=0.8989833333333334\n"
     ]
    }
   ],
   "source": [
    "net.train(PrepareMNISTData.get_training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "5bdc34fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss=0.3059302164982831, accuracy=0.8844\n"
     ]
    }
   ],
   "source": [
    "net.evaluate(PrepareMNISTData.get_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6579fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

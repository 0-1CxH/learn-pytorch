{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c16463a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "528ceb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(torch.Tensor([1,2,3,4]), requires_grad=True) # requires_grad: need autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb3a64a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3., 4.]) torch.Size([4]) None None\n"
     ]
    }
   ],
   "source": [
    "print(x.data, x.shape, x.grad_fn, x.grad) \n",
    "# Variable.grad_fn: the creator function of the var; x comes from Tensor, so grad_fn is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d1edd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = Variable(torch.randn(2,4), True) # Variable is Tensor that from which computation graph can be created\n",
    "# torch.randn: random number from normal dist;\n",
    "# torch.rand: random number from uniform dist;\n",
    "# other methods: torch.ones, torch.zeros, torch.eye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9d0b98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2925,  0.0732, -0.9787, -1.2007],\n",
      "        [-0.5189,  0.6909,  2.7205,  1.0611]]) torch.Size([2, 4]) None None\n"
     ]
    }
   ],
   "source": [
    "print(y.data, y.shape, y.grad_fn, y.grad) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86d6a5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2925],\n",
      "        [ 0.0732],\n",
      "        [-0.9787],\n",
      "        [-1.2007],\n",
      "        [-0.5189],\n",
      "        [ 0.6909],\n",
      "        [ 2.7205],\n",
      "        [ 1.0611]], grad_fn=<ViewBackward0>)\n",
      "tensor([[-0.2925,  0.0732, -0.9787, -1.2007, -0.5189,  0.6909,  2.7205,  1.0611]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[-0.2925,  0.0732],\n",
      "        [-0.9787, -1.2007],\n",
      "        [-0.5189,  0.6909],\n",
      "        [ 2.7205,  1.0611]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(y.view(-1,1)) # Tensor.view: flatten the tensor and make new one by the dim args given (like numpy)\n",
    "print(y.view(1,8))\n",
    "print(y.view(4,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de59fda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91185715",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.ndarray((2,1))\n",
    "w = torch.from_numpy(w)  # torch.from_numpy: make a torch.Tensor object from numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d56e8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0583e-312],\n",
      "        [2.3342e-312]], dtype=torch.float64) <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(w.data, type(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5669a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available(): # if cuda gpu is avaiable,\n",
    "    w = w.cuda()  # Tensor.cuda: migrate tensor to cuda gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f198ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = w.cpu() # migrate w to cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4010caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0583e-312],\n",
      "        [2.3342e-312]], dtype=torch.float64) torch.Size([2, 1]) None None\n"
     ]
    }
   ],
   "source": [
    "print(w.data, w.shape, w.grad_fn, w.grad) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76e1ca6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wx = w * x\n",
    "# the other way to do the same thing is: wx=torch.multiply(w,x)\n",
    "# for inplace computation w = w * x is same as: w.multiply_(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ecc5e028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0583e-312, 4.1167e-312, 6.1750e-312, 8.2333e-312],\n",
      "        [2.3342e-312, 4.6684e-312, 7.0026e-312, 9.3368e-312]],\n",
      "       dtype=torch.float64) torch.Size([2, 4]) <MulBackward0 object at 0x105d8bd00> None\n"
     ]
    }
   ],
   "source": [
    "print(wx.data, wx.shape, wx.grad_fn, wx.grad) \n",
    "# wx is computed by w multiplies x, so the grad_fn is 'MulBackward'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "692a5a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_wx = torch.add(y, wx) # the same as: y_wx = y + wx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "974469d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2925,  0.0732, -0.9787, -1.2007],\n",
      "        [-0.5189,  0.6909,  2.7205,  1.0611]], dtype=torch.float64) torch.Size([2, 4]) <AddBackward0 object at 0x14d63a7f0> None\n"
     ]
    }
   ],
   "source": [
    "print(y_wx.data, y_wx.shape, y_wx.grad_fn, y_wx.grad)\n",
    "# y_wx id computed by adding y and wx, so the grad_fn is 'AddBackward'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a2ed1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad can be implicitly created only for scalar outputs\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    y_wx.backward() # try to compute the grad of y_wx\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "# the y_wx is tensor, not scalar(1*1 tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9baf061",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_wx.backward(torch.ones_like(y_wx), retain_graph=True)\n",
    "# 'retain_graph': retain the computation graph of y_wx to backward through the graph a second time\n",
    "# torch.ones_like: make a all-one tensor the same shape as the input arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ece43f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0.]) tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad, y.grad) # get grad of x and y, i.e. d(y_wx)/dx, d(y_wx)/dy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b98e925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(w.grad)  # w is not set as 'requires_grad'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16ee363d",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_y_wx = y_wx.sum() # the other way of making y_wx a scalar is summing up the y_wx\n",
    "s_y_wx.backward() # the computation graph of y_wx is retained; else, an exception will be raised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af49990f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0.]) tensor([[2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2.]])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad, y.grad) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

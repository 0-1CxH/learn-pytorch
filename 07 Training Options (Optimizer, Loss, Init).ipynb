{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "773b110e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4f1814b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) Some Pre-built Options in torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f7f894f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ASGD', 'Adadelta', 'Adagrad', 'Adam', 'AdamW', 'Adamax', 'LBFGS', 'NAdam', 'Optimizer', 'RAdam', 'RMSprop', 'Rprop', 'SGD', 'SparseAdam', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '_functional', '_multi_tensor', 'lr_scheduler', 'swa_utils']\n"
     ]
    }
   ],
   "source": [
    "print(dir(torch.optim))  # torch.optim contains the common optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97a818dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AdaptiveLogSoftmaxWithLoss', 'BCELoss', 'BCEWithLogitsLoss', 'CTCLoss', 'CosineEmbeddingLoss', 'CrossEntropyLoss', 'GaussianNLLLoss', 'HingeEmbeddingLoss', 'HuberLoss', 'KLDivLoss', 'L1Loss', 'MSELoss', 'MarginRankingLoss', 'MultiLabelMarginLoss', 'MultiLabelSoftMarginLoss', 'MultiMarginLoss', 'NLLLoss', 'PoissonNLLLoss', 'SmoothL1Loss', 'SoftMarginLoss', 'TripletMarginLoss', 'TripletMarginWithDistanceLoss']\n"
     ]
    }
   ],
   "source": [
    "print([_ for _ in dir(torch.nn) if _.endswith(\"Loss\")])  # torch.nn contains the common loss fucntions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "669587c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_no_grad_normal_', '_no_grad_trunc_normal_', '_no_grad_uniform_', 'kaiming_normal', 'kaiming_normal_', 'kaiming_uniform', 'kaiming_uniform_', 'normal', 'normal_', 'trunc_normal_', 'uniform', 'uniform_', 'xavier_normal', 'xavier_normal_', 'xavier_uniform', 'xavier_uniform_']\n"
     ]
    }
   ],
   "source": [
    "print([_ for _ in dir(torch.nn.init) if (\"uniform\" in _ or \"normal\" in _)]) \n",
    "# torch.nn contains the common init functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10dd3425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) Details of Training Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45ed0261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a sample network for later sections\n",
    "import numpy as np\n",
    "\n",
    "class SampleNetwork(object):\n",
    "    \"\"\" \n",
    "    input=2\n",
    "    tanh_layer input=2, output=4\n",
    "    tanh_layer input=4, output=1\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 lr=5e-2, \n",
    "                 epoch=10,):\n",
    "        self.network = torch.nn.Sequential(\n",
    "            torch.nn.Linear(2,4),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(4,1),\n",
    "            torch.nn.Tanh(),\n",
    "        ) \n",
    "        \n",
    "        self.epoch = epoch\n",
    "        \n",
    "        self.optimizer = None\n",
    "        self.loss_func = torch.nn.BCEWithLogitsLoss()\n",
    "        \n",
    "    @staticmethod\n",
    "    def make_dataset(sample_count, input_dim=2, label_classes=2):\n",
    "        x = np.zeros((sample_count, input_dim))\n",
    "        y = np.zeros((sample_count, 1))\n",
    "        N = int(sample_count/label_classes)\n",
    "        for c in range(label_classes):\n",
    "            ix = range(N*c,N*(c+1))\n",
    "            t = np.linspace(c*3.12,(c+1)*3.12,N) + np.random.randn(N)*0.2\n",
    "            r = 4*np.sin(4*t) + np.random.randn(N)*0.2\n",
    "            x[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
    "            y[ix] = c\n",
    "        return torch.Tensor(x), torch.Tensor(y)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def train_by_step(self, amount=10):\n",
    "        x_train, y_train = SampleNetwork.make_dataset(amount)\n",
    "        for e in range(1):\n",
    "            y_estimate = self.network(x_train)\n",
    "            loss = self.loss_func(y_estimate, y_train) # attention: do not swap the position!\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            if e%100==0 or e==self.epoch-1:\n",
    "                print(\"epoch{}/{}, loss={}\".format(e, self.epoch, loss))\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "569c0b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2])\n",
      "torch.Size([4])\n",
      "torch.Size([1, 4])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "s = SampleNetwork()\n",
    "for _ in s.network.parameters():\n",
    "    print(_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d317d82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2.1) Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0f65204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this section, SGD / MomentumSGD / Nesterov, Adagrad, RMSProp, Adam are introduced with usage and pseudo-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d4f2d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'state': {}, 'param_groups': [{'lr': 0.01, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': True, 'params': [0, 1, 2, 3]}]}\n"
     ]
    }
   ],
   "source": [
    "# (2.1.1)\n",
    "# SGD / MomentumSGD / Nesterov\n",
    "s = SampleNetwork()\n",
    "s.optimizer = torch.optim.SGD(s.network.parameters(), lr=1e-2, momentum=0.9, nesterov=True)\n",
    "\n",
    "# SGD optimizer internal states\n",
    "print(s.optimizer.state_dict())\n",
    "\n",
    "# 'state' comtains the momentum states (velocity)\n",
    "# 'param_groups' contains lr, momentum factor, nestrov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c76f922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0/10, loss=0.6975148916244507\n",
      "{'state': {0: {'momentum_buffer': tensor([[-0.0024,  0.0248],\n",
      "        [ 0.0096, -0.0514],\n",
      "        [-0.0011, -0.0366],\n",
      "        [ 0.0034, -0.0025]])}, 1: {'momentum_buffer': tensor([-0.0104,  0.0036,  0.0122,  0.0003])}, 2: {'momentum_buffer': tensor([[ 0.0965, -0.1078,  0.1613, -0.0337]])}, 3: {'momentum_buffer': tensor([0.0682])}}, 'param_groups': [{'lr': 0.01, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': True, 'params': [0, 1, 2, 3]}]}\n"
     ]
    }
   ],
   "source": [
    "s.train_by_step()\n",
    "print(s.optimizer.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "635fb49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pesudo-code of sgd/momentumSgd/nestrov updating:\n",
    "\n",
    "#  (if nestorv: w += m*v)\n",
    "# v = m*v+lr*grad\n",
    "# w -= v\n",
    "\n",
    "def sgd_step(network_parameters):\n",
    "    optimizer_state_dict = get_state_dict()\n",
    "    params_of_current_step = optimizer_state_dict['param_groups']\n",
    "    for layerIndex, layerParam in enumerate(network_parameters):\n",
    "        state_of_current_layer = optimizer_state_dict['state'][layerIndex]\n",
    "        \n",
    "        if params_of_current_step['nesterov']: # apply nestorv temporary update\n",
    "            layerParam += params_of_current_step['momentum'] * state_of_current_layer['momentum_buffer']\n",
    "        \n",
    "        state_of_current_layer['momentum_buffer'] = ( \n",
    "            params_of_current_step['momentum'] * state_of_current_layer['momentum_buffer'] + params_of_current_step['lr'] * layerParam.grad.data\n",
    "        )\n",
    "        layerParam -= state_of_current_layer['momentum_buffer']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c53cbac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'state': {0: {'step': 0, 'sum': tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]])}, 1: {'step': 0, 'sum': tensor([0., 0., 0., 0.])}, 2: {'step': 0, 'sum': tensor([[0., 0., 0., 0.]])}, 3: {'step': 0, 'sum': tensor([0.])}}, 'param_groups': [{'lr': 0.01, 'lr_decay': 0, 'eps': 1e-10, 'weight_decay': 0, 'initial_accumulator_value': 0, 'params': [0, 1, 2, 3]}]}\n"
     ]
    }
   ],
   "source": [
    "# (2.1.2)\n",
    "# Adagrad\n",
    "s = SampleNetwork()\n",
    "s.optimizer = torch.optim.Adagrad(s.network.parameters(), lr=1e-2)\n",
    "\n",
    "# adagrad optimizer internal states\n",
    "print(s.optimizer.state_dict())\n",
    "\n",
    "# 'state' contains the sum of square of previous grads\n",
    "# 'param_groups' contains lr, eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93472678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0/10, loss=0.6869338154792786\n",
      "{'state': {0: {'step': 1, 'sum': tensor([[0.0013, 0.0011],\n",
      "        [0.0007, 0.0004],\n",
      "        [0.0003, 0.0017],\n",
      "        [0.0010, 0.0004]])}, 1: {'step': 1, 'sum': tensor([2.8363e-05, 9.4177e-04, 3.8914e-05, 2.6537e-04])}, 2: {'step': 1, 'sum': tensor([[0.0026, 0.0003, 0.0038, 0.0017]])}, 3: {'step': 1, 'sum': tensor([0.0064])}}, 'param_groups': [{'lr': 0.01, 'lr_decay': 0, 'eps': 1e-10, 'weight_decay': 0, 'initial_accumulator_value': 0, 'params': [0, 1, 2, 3]}]}\n"
     ]
    }
   ],
   "source": [
    "s.train_by_step()\n",
    "print(s.optimizer.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e3e172e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0/10, loss=0.66921466588974\n",
      "{'state': {0: {'step': 2, 'sum': tensor([[0.0017, 0.0015],\n",
      "        [0.0009, 0.0044],\n",
      "        [0.0003, 0.0023],\n",
      "        [0.0017, 0.0006]])}, 1: {'step': 2, 'sum': tensor([0.0003, 0.0015, 0.0001, 0.0007])}, 2: {'step': 2, 'sum': tensor([[0.0026, 0.0005, 0.0039, 0.0040]])}, 3: {'step': 2, 'sum': tensor([0.0118])}}, 'param_groups': [{'lr': 0.01, 'lr_decay': 0, 'eps': 1e-10, 'weight_decay': 0, 'initial_accumulator_value': 0, 'params': [0, 1, 2, 3]}]}\n"
     ]
    }
   ],
   "source": [
    "s.train_by_step()\n",
    "print(s.optimizer.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be860abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pesudo-code of adagrad updating\n",
    "\n",
    "# s += grad^2\n",
    "# w -= grad * lr/sqrt(s+eps)\n",
    "\n",
    "def adagrad_step(network_parameters):\n",
    "    optimizer_state_dict = get_state_dict()\n",
    "    params_of_current_step = optimizer_state_dict['param_groups']\n",
    "    for layerIndex, layerParam in enumerate(network_parameters):\n",
    "        state_of_current_layer = optimizer_state_dict['state'][layerIndex]\n",
    "        state_of_current_layer['step'] += 1\n",
    "        state_of_current_layer['sum'] += layerParam.grad.data ** 2  \n",
    "        \n",
    "        scaled_lr = params_of_current_step['lr'] / torch.sqrt(state_of_current_layer['sum'] + params_of_current_step['eps'])\n",
    "        layerParam -= scaled_lr * layerParam.grad.data\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ed23b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'state': {}, 'param_groups': [{'lr': 0.01, 'momentum': 0, 'alpha': 0.9, 'eps': 1e-08, 'centered': False, 'weight_decay': 0, 'params': [0, 1, 2, 3]}]}\n"
     ]
    }
   ],
   "source": [
    "# (2.1.3)\n",
    "# RMSProp\n",
    "s = SampleNetwork()\n",
    "s.optimizer = torch.optim.RMSprop(s.network.parameters(), lr=1e-2, alpha=0.9)\n",
    "\n",
    "# adagrad optimizer internal states\n",
    "print(s.optimizer.state_dict())\n",
    "\n",
    "# 'state' contains the Exponential Moving Average (EMA)\n",
    "# 'param_groups' contains lr, eps, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c082da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0/10, loss=0.6510215997695923\n",
      "{'state': {0: {'step': 1, 'square_avg': tensor([[4.5999e-05, 2.3468e-05],\n",
      "        [6.1505e-05, 2.4501e-05],\n",
      "        [1.3273e-06, 2.5026e-06],\n",
      "        [1.7386e-06, 9.7919e-07]])}, 1: {'step': 1, 'square_avg': tensor([1.4462e-05, 1.5376e-05, 5.0655e-07, 4.1954e-07])}, 2: {'step': 1, 'square_avg': tensor([[8.0399e-04, 1.5824e-04, 1.6765e-04, 1.5830e-05]])}, 3: {'step': 1, 'square_avg': tensor([0.0007])}}, 'param_groups': [{'lr': 0.01, 'momentum': 0, 'alpha': 0.9, 'eps': 1e-08, 'centered': False, 'weight_decay': 0, 'params': [0, 1, 2, 3]}]}\n"
     ]
    }
   ],
   "source": [
    "s.train_by_step()\n",
    "print(s.optimizer.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f1fe629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pesudo-code of rmsprop updating\n",
    "\n",
    "# ema = ema*alpha + grad^2*(1-alpha)  # this step is the only difference from Adagrad\n",
    "# w -= grad * lr/sqrt(ema+eps)\n",
    "\n",
    "def rmsprop_step(network_parameters):\n",
    "    optimizer_state_dict = get_state_dict()\n",
    "    params_of_current_step = optimizer_state_dict['param_groups']\n",
    "    alpha = params_of_current_step['alpha']\n",
    "    for layerIndex, layerParam in enumerate(network_parameters):\n",
    "        state_of_current_layer = optimizer_state_dict['state'][layerIndex]\n",
    "        state_of_current_layer['step'] += 1\n",
    "        state_of_current_layer['square_avg'] = alpha * state_of_current_layer['square_avg'] + (1-alpha) * layerParam.grad.data ** 2  \n",
    "        \n",
    "        scaled_lr = params_of_current_step['lr'] / torch.sqrt(state_of_current_layer['sum'] + params_of_current_step['eps'])\n",
    "        layerParam -= scaled_lr * layerParam.grad.data\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac8b2ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'state': {}, 'param_groups': [{'lr': 0.01, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'params': [0, 1, 2, 3]}]}\n"
     ]
    }
   ],
   "source": [
    "# (2.1.4)\n",
    "# Adam\n",
    "s = SampleNetwork()\n",
    "s.optimizer = torch.optim.Adam(s.network.parameters(), lr=1e-2, betas=[0.9, 0.999])\n",
    "\n",
    "# adagrad optimizer internal states\n",
    "print(s.optimizer.state_dict())\n",
    "\n",
    "# 'state' contains the Exponential Moving Average (EMA) and square of EMA\n",
    "# 'param_groups' contains lr, eps, betas(beta1, beta2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "891150be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0/10, loss=0.7266576886177063\n",
      "{'state': {0: {'step': 1, 'exp_avg': tensor([[-0.0007, -0.0016],\n",
      "        [-0.0072, -0.0058],\n",
      "        [-0.0036, -0.0059],\n",
      "        [ 0.0066,  0.0045]]), 'exp_avg_sq': tensor([[4.3446e-08, 2.4981e-07],\n",
      "        [5.2516e-06, 3.3817e-06],\n",
      "        [1.3294e-06, 3.4928e-06],\n",
      "        [4.3454e-06, 1.9908e-06]])}, 1: {'step': 1, 'exp_avg': tensor([-0.0002, -0.0057, -0.0029,  0.0017]), 'exp_avg_sq': tensor([4.6351e-09, 3.2998e-06, 8.1780e-07, 2.7935e-07])}, 2: {'step': 1, 'exp_avg': tensor([[ 0.0131, -0.0003, -0.0064, -0.0078]]), 'exp_avg_sq': tensor([[1.7275e-05, 6.5885e-09, 4.1488e-06, 6.1445e-06]])}, 3: {'step': 1, 'exp_avg': tensor([-0.0065]), 'exp_avg_sq': tensor([4.2628e-06])}}, 'param_groups': [{'lr': 0.01, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'params': [0, 1, 2, 3]}]}\n"
     ]
    }
   ],
   "source": [
    "s.train_by_step()\n",
    "print(s.optimizer.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4d034fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pesudo-code of adam updating\n",
    "\n",
    "# ema = ema*beta1 + grad*(1-beta1)\n",
    "# ema_sqr = ema_sqr*beta2 + grad**2 * (1-beta2)\n",
    "# w -= (lr * ema/(1-beta1)) / sqrt(ema_sqr/(1-beta2) + eps)\n",
    "\n",
    "# Adam is actually momentum + rmsprop\n",
    "\n",
    "# lr * ema/(1-beta1) = ema*[lr*beta1/(1-beta1)] + grad*lr\n",
    "# this is the same as momentum step: \"v = m*v + lr*grad\"\n",
    "\n",
    "# ema_sqr updating in Adam is the same as RMSProp step : \"ema = ema*alpha + grad^2*(1-alpha)\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e53b5041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sgd -- momentum -----\\\n",
    "#                       ---Adam\n",
    "# adagrad -- rmsprop --|    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a5f7786b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2.2) Loss Functions\n",
    "# In this section, BCE/BECLogit, CE, L1, L2/MSE, SmoothL1 loss are introduced with usage and pesudo-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "17d1d6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "single_class_data = (torch.Tensor([.8, .7, .5, .3]), torch.Tensor([1.0, .0, 1.0, .0]))\n",
    "multi_class_data = (torch.Tensor([[.8, .1, .1], [.2, .5, .3], [.1, .2, .7]]), \n",
    "                         torch.Tensor([0, 1, 1]).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e56bf489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2231, 1.2040, 0.6931, 0.3567])\n"
     ]
    }
   ],
   "source": [
    "# (2.2.1) BCE loss (Single Class)\n",
    "loss = torch.nn.BCELoss(reduction='none') # set reduction='mean' in real use\n",
    "# BCE(x,y) = -( ylog(x) + (1-y)log(1-x) )\n",
    "# 当我们进行的是二分类时，即激活函数使用的是sigmoid函数时，常使用交叉熵作为损失函数。这样就能够解决因sigmoid函数导致的梯度消失问题\n",
    "# (MSE有这个问题， BCE解决了)\n",
    "\n",
    "print(loss(*single_class_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "30e0982b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2231, 1.2040, 0.6931, 0.3567])\n"
     ]
    }
   ],
   "source": [
    "def bce_loss(x, y):\n",
    "    return - ( y *  torch.log(x) + (1-y) * torch.log(1-x) )\n",
    "\n",
    "print(bce_loss(*single_class_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1f093594",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1113, 1.1032, 0.1896, 0.8544])\n"
     ]
    }
   ],
   "source": [
    "# (2.2.2) BCE with Logit Loss (= sigmoid + BCE)\n",
    "loss = torch.nn.BCEWithLogitsLoss(reduction='none' , pos_weight=torch.Tensor([0.3, 0.2, 0.4, 0.1]))\n",
    "# BCELL(x,y) = - (ylog(sigmoid(x)) + (1-y)log(sigmoid(1-x)))\n",
    "# pos_weight: if real_y==1, loss * pos_weight, else pass\n",
    "print(loss(*single_class_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fcf289f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6897, 0.9398, 1.2679])\n"
     ]
    }
   ],
   "source": [
    "# (2.2.3) CE Loss (Multiple Classes) (=NLL + LogSoftmax)\n",
    "loss = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "# CE(x,class) = -log( exp(x[class])/sum(exp(x[i]) for i) ) \n",
    "\n",
    "print(loss(*multi_class_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "86f3c442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6897266357726802, 0.939831065520222, 1.267949541610605]\n"
     ]
    }
   ],
   "source": [
    "def ce_loss(x, y):\n",
    "    for xi, yi in zip(x,y):\n",
    "        s = 0\n",
    "        c = 0\n",
    "        for j, xij in enumerate(xi):\n",
    "            s += math.exp(xij)\n",
    "            if yi==j:\n",
    "                c = math.exp(xij)\n",
    "        yield - math.log(c/s)\n",
    "\n",
    "print([_ for _ in ce_loss(*multi_class_data)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1ba38f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.8000, -0.5000, -0.2000])\n"
     ]
    }
   ],
   "source": [
    "# (2.2.4) NLL Loss (Multiple Classes)\n",
    "loss = torch.nn.NLLLoss(reduction='none')\n",
    "# NLLLoss(x,class) = -x[class]\n",
    "\n",
    "print(loss(*multi_class_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "66b257fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(-0.8000), tensor(-0.5000), tensor(-0.2000)]\n"
     ]
    }
   ],
   "source": [
    "def nll_loss(x, y):\n",
    "    for xi, yi in zip(x,y):\n",
    "        yield -xi[yi]\n",
    "        \n",
    "print(list(nll_loss(*multi_class_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3835655d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2000, 0.7000, 0.5000, 0.3000])\n"
     ]
    }
   ],
   "source": [
    "# (2.2.5) L1Norm Loss (Regression)\n",
    "loss = torch.nn.L1Loss(reduction='none')\n",
    "# L1(x, y)= |x-y|\n",
    "print(loss(*single_class_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "59a65f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0400, 0.4900, 0.2500, 0.0900])\n"
     ]
    }
   ],
   "source": [
    "# (2.2.6) L2Norm Loss / MSE (Regression)\n",
    "loss = torch.nn.MSELoss(reduction='none')\n",
    "# L2(x, y) = (x-y)**2\n",
    "print(loss(*single_class_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cbca17c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0200, 0.2450, 0.1250, 0.0450])\n"
     ]
    }
   ],
   "source": [
    "# (2.2.7) SmoothL1 (Regression)\n",
    "loss = torch.nn.SmoothL1Loss(reduction='none')\n",
    "# if |x-y|<1, SmoothL1(x, y)=0.5*L2(x,y), else, = L1(x-y)-0.5\n",
    "print(loss(*single_class_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b0b2ce21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The most usual usage:\n",
    "\n",
    "# Single class: BCELogit(=sigmoid+BCE)\n",
    "# Multiple classes: CE(=log+softmax+NLL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57aeb69c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
